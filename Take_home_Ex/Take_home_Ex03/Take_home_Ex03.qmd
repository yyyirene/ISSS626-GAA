---
title: "Take home Exercise03"
author: "yang yayong"
date: "Oct 24 2024"
date-modified: "last-modified"
execute:
  eval: true
  echo: true
  message: false
  freeze: true
  timeout: 1200 
---

## Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods

![](截屏2020-12-21-下午3.18.14.png){width="692"}

::: panel-tabset
## **Setting the Scene**

Housing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

Conventional, housing resale prices predictive models were built by using Ordinary Least Square (OLS) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, **Geographical Weighted Models** were introduced to better calibrate predictive models for housing resale prices.

In this study, we focus on key residential areas—Jurong East, Woodlands, Yishun, Tampines, Kallang, and Queenstown—alongside [HDB projects for 2024](https://www.hdb.gov.sg/about-us/news-and-publications/press-releases/HDB-Launches-6938-Flats-in-June-2024-BTO-Exercise). This targeted approach aims to offer valuable insights for prospective homebuyers, helping them make well-informed decisions and better prepare for the housing market.

## **The Task**

In this take-home exercise, we are required to calibrate a predictive model to predict HDB resale prices between July-September 2024 by using HDB resale transaction records in 2023.
:::

## 1.The Data

-   **Aspatial dataset**:

    -   HDB Resale data: a list of HDB resale transacted prices in Singapore from Jan 2017 onwards. It is in csv format which can be downloaded from Data.gov.sg.

-   **Geospatial dataset**:

    -   *MP14_SUBZONE_WEB_PL*: a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg

-   **Locational factors with geographic coordinates**:

    -   Downloaded from **Data.gov.sg**.

        -   **Eldercare** data is a list of eldercare in Singapore. It is in shapefile format.

        -   **Hawker Centre** data is a list of hawker centres in Singapore. It is in geojson format.

        -   **Parks** data is a list of parks in Singapore. It is in geojson format.

        -   **Supermarket** data is a list of supermarkets in Singapore. It is in geojson format.

        -   **CHAS clinics** data is a list of CHAS clinics in Singapore. It is in geojson format.

        -   **Kindergartens** data is a list of kindergartens in Singapore. It is in geojson format.

    ::: callout-note
    If we need to display the prediction results on a web application or map, GeoJSON is more convenient because most web map libraries (such as Leaflet and Mapbox) directly support GeoJSON data.

    When visualizing the analysis results, we can directly export them to GeoJSON, which is convenient for displaying the prediction results on various online map applications.
    :::

    -   Downloaded from **Datamall.lta.gov.sg**.

        -   **MRT** data is a list of MRT/LRT stations in Singapore with the station names and codes. It is in shapefile format.

        -   **Bus stops** data is a list of bus stops in Singapore. It is in shapefile format.

-   **Locational factors without geographic coordinates**:

    -   Retrieved/Scraped from **other sources**

        -   **CBD** coordinates obtained from Google.

        -   **Shopping malls** data is a list of Shopping malls in Singapore obtained from [Wikipedia](https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore).

        -   **Good primary schools** is a list of primary schools that are ordered in ranking in terms of popularity and this can be found at [Local Salary Forum](https://www.salary.sg/2021/best-primary-schools-2021-by-popularity).

## 2.Getting start

### Installing and Loading R packages

```{r}
pacman::p_load(sf, spdep, GWmodel, SpatialML, tmap, rsample, Metrics, tidyverse,stringr,httr, jsonlite, rvest,knitr,kableExtra)
```

### Importing Resale Data

Using HDB resale transaction records in 2023 and the future data from July to September for comparison.

```{r}
resale <- read_csv("data/rawdata/aspatial/resale.csv") %>%
  filter((month >= "2023-01" & month < "2024-01") | 
         (month >= "2024-07" & month <= "2024-09"))
```

### Data Processing for HDB Resale

The specific steps are as follows:

-   Create a new column address: Combine the address information into a new column address by splicing the block and street_name columns.

-   Extract and convert the remaining lease time: Extract the year and month from the remaining_lease column and create integer columns remaining_lease_yr and remaining_lease_mth to represent the remaining lease years and months respectively.

-   Remove columns that are no longer needed: Delete the block, street_name, and remaining_lease columns because they have been integrated or decomposed into other variables.

-   Clean the flat_type column: Remove the word "ROOM" in the flat_type column and represent the room type as a pure number.

-   Convert the resale_price column to an integer: Convert the data type of the resale price resale_price column to an integer.

-   Separate the storey_range column: Split the floor range information from the storey_range column into two columns, min_storey and max_storey, and convert them to integer type.

-   Calculate the middle value of the floor: Create a storey_mid column, calculate the average of min_storey and max_storey, and represent the middle number of the floor.

-   Remove redundant floor columns: Delete the min_storey and max_storey columns because their middle values ​​are already represented by the storey_mid column.

```{r}
resale_tidy <- resale %>%
  mutate(address = paste(block,street_name)) %>% 
  mutate(remaining_lease_yr = as.integer(    
    str_sub(remaining_lease, 0, 2)))%>%  
  mutate(remaining_lease_mth = as.integer(
    str_sub(remaining_lease, 9, 11))) %>% 
  mutate(total_remaining_months = remaining_lease_yr * 12 + remaining_lease_mth)  %>% 
  select(-block, -street_name,-remaining_lease,-remaining_lease_yr,-remaining_lease_mth) %>% 
  mutate(flat_type = str_replace(flat_type, " ROOM", "")) %>% 
  mutate(resale_price = as.integer(resale_price)) %>% 
  separate(storey_range, into = c("min_storey", "max_storey"), sep = " TO ", convert = TRUE) %>%
  mutate(storey_mid = (min_storey + max_storey) / 2) %>%
  select(-min_storey, -max_storey)     

```

### Convert into categorical variables

Factor is a data type used to represent categorical variables. And they can store discrete, finite categories.

```{r}
resale_tidy$town <- as.factor(resale_tidy$town)
```

See how many categories of the flat_type.

```{r}
unique(resale_tidy$flat_type)
```

See how many categories of the flat_model.

```{r}
unique(resale_tidy$flat_model)
```

### Numerical categorical variables for flat_model.

```{r}
resale_tidy$flat_model_numeric <- as.numeric(factor(resale_tidy$flat_model))

factor_levels <- levels(factor(resale_tidy$flat_model))
mapping <- data.frame(Number = 1:length(factor_levels), Model = factor_levels)
print(mapping)
```

### Numerical categorical variables flat_type.

```{r}
resale_tidy$flat_type_numeric <- as.numeric(factor(resale_tidy$flat_type))

factor_levels2 <- levels(factor(resale_tidy$flat_type))
mapping <- data.frame(Number = 1:length(factor_levels2), Model = factor_levels2)
print(mapping)
```

```{r}
resale_tidy<-resale_tidy %>% select(-flat_type,-flat_model)
```

::: callout-note
After conversion to factors, the model can generate different coefficients for each category, which makes it easier to interpret the specific impact of each category on the result. For example, for a variable like "flat_model" (house model), the model can provide the impact of each house model on price, rather than simply encoding the house model.
:::

### Getting coords

We firstly extract the unique address from dataset,which will be more quickly to get the result.

```{r}
#| eval: false
add_list <- sort(unique(resale_tidy$address))
```

Using the code provided by Professor Kam ,which created the function to get the coordinates from onemap website.

```{r}
#| eval: false
#| code-fold: true
#| code-summary: "Show the code"
get_coords <- function(add_list){
  
  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (i in add_list){
    #print(i)

    r <- GET('https://www.onemap.gov.sg/api/common/elastic/search?',
           query=list(searchVal=i,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(address= i, 
                            postal = postal, 
                            latitude = lat, 
                            longitude = lng)
    }
    
    # If multiple results, drop NIL and append top 1
    else if (found > 1){
      # Remove those with NIL as postal
      res_sub <- res[res$POSTAL != "NIL", ]
      
      # Set as NA first if no Postal
      if (nrow(res_sub) == 0) {
          new_row <- data.frame(address= i, 
                                postal = NA, 
                                latitude = NA, 
                                longitude = NA)
      }
      
      else{
        top1 <- head(res_sub, n = 1)
        postal <- top1$POSTAL 
        lat <- top1$LATITUDE    # return x and y
        lng <- top1$LONGITUDE
        new_row <- data.frame(address= i, 
                              postal = postal, 
                              latitude = lat, 
                              longitude = lng)
      }
    }

    else {
      new_row <- data.frame(address= i, 
                            postal = NA,  
                            latitude = NA, # take care of any possible data 
                            longitude = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  return(postal_coords)
}
```

::: callout-note
In this exercise,we mainly use the detail address to get the coordinate and there is another method,which is using the postal,and the result is also the similar.
:::

Start to get coordinates of each asset.

```{r}
#| eval: false
coords_resale2 <- get_coords(add_list)
```

Next saving and reading the data for later convenient analysis

```{r}
#| eval: false
saveRDS(coords_resale2, "data/rds/coords_resale2.rds")
```

```{r}
coords_resale2 <- readRDS("data/rds/coords_resale2.rds")
```

```{r}
#| eval: false
saveRDS(coords_resale, "data/rds/coords_resale.rds")
```

```{r}
coords_resale <- readRDS("data/rds/coords_resale.rds")
```

### Integrate data by using left join

```{r}
resale_tidy <- resale_tidy %>%
  left_join(coords_resale2, by = "address")
```

Delete the useless postal columns

```{r}
resale_tidy2 <- resale_tidy %>%
  select(-postal) 
```

Here we check that there is no coordinate information in the dataset ,so we manually add on and transfer into Singapore coordinate.

```{r}
print(st_crs(resale_tidy2))
```

```{r}
resale_tidy2 <- st_as_sf(resale_tidy2, coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform(crs = 3414)
```

Now we can see it is successfully converted.

```{r}
st_crs(resale_tidy2)
```

### Importing Geospatial Data

```{r}
mpsz <- st_read(dsn = "data/rawdata/geospatial", 
                layer = "MP14_SUBZONE_WEB_PL")
```

### Importing Locational factors :

**Importing Locational factors with geographic coordinates**

```{r}
eldercare <- st_read(dsn = "data/rawdata/geospatial", layer = "ELDERCARE")   #meter

hawker_centres <- st_read("data/rawdata/geospatial/HawkerCentresGEOJSON.geojson")

kindergartens <- st_read("data/rawdata/geospatial/Kindergartens.geojson")

parks <- st_read("data/rawdata/geospatial/Parks.geojson")

supermarkets <- st_read("data/rawdata/geospatial/SupermarketsGEOJSON.geojson")

chas_clinics <- st_read("data/rawdata/geospatial/CHASclinics.geojson") 

BusStop <- st_read(dsn = "data/rawdata/geospatial", layer = "BusStop") 

MRT <- st_read(dsn = "data/rawdata/geospatial", layer = "RapidTransitSystemStation") 
```

Convert a multipoint object (MULTIPOINT) to a single point (POINT) ,which is convenient for subsequent calculations

```{r}
MRT <- st_cast(MRT, "POINT")
```

**Importing Locational factors without geographic coordinates**

```{r}
Goodprimaryschool <- read_csv("data/rawdata/aspatial/Goodprimaryschool.csv") 
```

```{r}
Shoppingmalls <- read_csv("data/rawdata/aspatial/Shoppingmalls.csv")
```

::: panel-tabset
### Getting coords for Good primary school

```{r}
add_list<- sort(unique(Goodprimaryschool$school_name))
```

```{r}
#| eval: false
coords_Goodprimaryschool <- get_coords(add_list) 
```

```{r}
#| eval: false
saveRDS(coords_Goodprimaryschool, "data/rds/coords_Goodprimaryschool.rds")
```

```{r}
coords_Goodprimaryschool <- readRDS("data/rds/coords_Goodprimaryschool.rds")
```

```{r}
Goodprimaryschool <- st_as_sf(coords_Goodprimaryschool, coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform(crs = 3414)
```

```{r}
st_crs(Goodprimaryschool)
```

### Getting coords for Shopping malls

```{r}
add_list<- sort(unique(Shoppingmalls$malladdress))
```

```{r}
#| eval: false
coords_Shoppingmalls <- get_coords(add_list)
```

```{r}
#| eval: false
saveRDS(coords_Shoppingmalls, "data/rds/coords_Shoppingmalls.rds")
```

```{r}
coords_Shoppingmalls <- readRDS("data/rds/coords_Shoppingmalls.rds")
```

```{r}
Shoppingmalls <- st_as_sf(coords_Shoppingmalls, coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform(crs = 3414)
```

```{r}
st_crs(Shoppingmalls)
```
:::

### Data Wrangling

**Apply Spatial Jitter**:

::: callout-note
When using GWmodel to calibrate explanatory or predictive models, it is very important to ensure that there are no overlapping point features

By adding a slight jitter, we make the points more distinguishable in maps or plots.
:::

The code chunk below is used to check if there are overlapping point features.

```{r}
#| eval: false
overlapping_points <- resale_tidy2 %>%
  mutate(overlap = lengths(st_equals(., .)) > 1)
```

From the result ,there are indeed overlapping points in the dataset.

So in the code chunk below, [`st_jitter()`](https://r-spatial.github.io/sf/reference/st_jitter.html) of **sf** package is used to move the point features by 0.05m to avoid overlapping point features.

```{r}
resale_tidy2 <- resale_tidy2 %>% st_jitter(amount = .05)
```

According to the Google provided,the Singapore's CBD coordinate and we transfer into Singapore projection

```{r}
cbd <- st_sfc(st_point(c(103.851959, 1.283850)), crs = 3414)
```

**Calculate Distances**: Use `st_distance()` to calculate the Euclidean distance between each property and the target locations.

```{r}
elders <- st_transform(eldercare, crs = 3414)
hawkers <- st_transform(hawker_centres, crs = 3414)
parks <- st_transform(parks, crs = 3414)
supermarkets <- st_transform(supermarkets, crs = 3414)
chas_clinics <- st_transform(chas_clinics, crs = 3414)
MRT <- st_transform(MRT, crs = 3414)
```

### Compute the distance between properties and facilities:

```{r}
#| eval: false
resale_tidy3 <- resale_tidy2 %>%
  mutate(
    dist_to_elders = st_distance(., elders) %>% apply(1, min),
    dist_to_hawkers = st_distance(., hawkers) %>% apply(1, min),
    dist_to_parks = st_distance(., parks) %>% apply(1, min),
    dist_to_supermarkets = st_distance(., supermarkets) %>% apply(1, min),
    dist_to_chas_clinics = st_distance(., chas_clinics) %>% apply(1, min),
    dist_to_cbd = st_distance(., cbd) %>% apply(1, min),
    dist_to_Shoppingmalls = st_distance(.,Shoppingmalls ) %>% apply(1, min),
    dist_to_MRT = st_distance(.,MRT ) %>% apply(1, min),
    dist_to_Goodprimaryschool = st_distance(.,Goodprimaryschool ) %>% apply(1, min)
    
  )
```

Now we can see the result as below :

![](Distance.png)

Note that the units are all meter.

Next saving and reading the data for later convenient analysis

```{r}
#| eval: false
saveRDS(resale_tidy3, "data/rds/resale_tidy3.rds")
```

```{r}
resale_tidy3 <- readRDS("data/rds/resale_tidy3.rds")
```

### Select the target area

```{r}
resale_tidy4 <- resale_tidy3 %>%
  filter(town %in% c("JURONG EAST", "WOODLANDS", "YISHUN", "TAMPINES", "KALLANG/WHAMPOA", "QUEENSTOWN")) %>% 
  filter(flat_type_numeric %in% c(3, 4, 5)) %>%
  select(-town, -address)
```

### Potting the target area

```{r}
tmap_options(check.and.fix = TRUE)

mpsz <- st_make_valid(mpsz)

study_areas <- c("JURONG EAST", "WOODLANDS", "YISHUN", "TAMPINES", "KALLANG", "QUEENSTOWN")

mpsz_filtered <- mpsz %>%
  filter(PLN_AREA_N %in% study_areas)

tmap_mode("plot")

tm_shape(mpsz) +
  tm_borders(col = "gray80", lwd = 0.5) +  
  tm_shape(mpsz_filtered) +
  tm_polygons(col = "PLN_AREA_N", palette = "Set3", border.col = "black") +  
  tm_text("PLN_AREA_N", size = 0.7, remove.overlap = TRUE, col = "black") +   
  tm_layout(title = "Map of Study Areas",
            legend.position = c("right", "bottom"))
```

### Counting numbers of facilities-point in Buffering area

**Buffering**

Before buffering,it is import to check and transform into the same crs.

```{r}
BusStop <- BusStop %>% st_transform(crs = 3414)
```

```{r}
kindergartens <- kindergartens %>% st_transform(crs = 3414)
```

```{r}
st_crs(kindergartens)
```

```{r}
st_crs(BusStop)
```

```{r}
buffer_350m <- st_buffer(resale_tidy4, dist = 350) 
buffer_1000m <- st_buffer(resale_tidy4, dist = 1000)
```

### Plot the newly created buffers and the assets.

```{r}
# Set tmap to static mode
tmap_mode("plot")

# Create the map with multiple layers
tm_shape(mpsz) +
  tm_borders() +
  tm_shape(buffer_350m) +
  tm_polygons() +
  tm_shape(resale_tidy4) +
  tm_dots()
```

```{r}
# Set tmap to static mode
tmap_mode("plot")

# Create the map with multiple layers
tm_shape(mpsz) +
  tm_borders() +
  tm_shape(buffer_1000m) +
  tm_polygons() +
  tm_shape(resale_tidy4) +
  tm_dots()
```

```{r,echo=FALSE,results='hide'}
st_crs(buffer_350m)
```

### Count number of points within a distance

```{r}
buffer_350m$BusStop_count <- lengths(st_intersects(buffer_350m, BusStop))
```

```{r,echo=FALSE,results='hide'}
st_crs(buffer_1000m)
```

```{r}
buffer_1000m$kindergartens_count<- lengths(st_intersects(buffer_1000m, kindergartens))
```

### Adding the new features into the dataset.

```{r}
resale_tidy5 <- resale_tidy4 %>%
  mutate(
    BusStop_within_350m = buffer_350m$BusStop_count,
    Kindergartens_within_1000m = buffer_1000m$kindergartens_count
  )
```

The result show as below:

![](pointcount.png){width="353"}

In this study,we first select the train and test data in 2023 for model training and also select the future data 2024 for later predict comparison.

Then we did not consider the time influence to the house price,and we remove the time variables.

```{r}
resale_tidy5_future <- resale_tidy5 %>%
  filter(month >= "2024-07" & month <= "2024-09") %>% select(-month)
```

```{r}
resale_tidy5 <- resale_tidy5 %>% filter(month >= "2023-01" & month < "2024-01") %>% select(-month)
```

```{r}
saveRDS(resale_tidy5, "data/rds/resale_tidy5.rds")
```

```{r}
resale_tidy5 <- readRDS("data/rds/resale_tidy5.rds")
```

## 3.Computing Correlation Matrix

Before loading the predictors into a predictive model, it is always a good practice to use correlation matrix to examine if there is sign of multicolinearity.

```{r}
resale_tidy5_nogeo <- resale_tidy5 %>%
  st_drop_geometry() 
```

Data type checking

```{r}
sapply(resale_tidy5_nogeo, class)
```

Replace NA values with 0 in both numeric and integer columns

```{r}
resale_tidy5_nogeo <- resale_tidy5_nogeo %>% 
  dplyr::mutate(across(where(~ is.numeric(.) || is.integer(.)), ~ replace_na(., 0)))
```

Generate correlation plot using only numeric and integer columns

```{r}
corrplot::corrplot(cor(resale_tidy5_nogeo), 
                   diag = FALSE, 
                   order = "AOE",
                   tl.pos = "td", 
                   tl.cex = 0.5, 
                   method = "number", 
                   type = "upper")
```

Find correlation coefficients greater than 0.8 or less than -0.8 (excluding the 1s on the diagonal)

```{r}
cor_matrix <- cor(resale_tidy5_nogeo)

high_corr <- which(abs(cor_matrix) > 0.8 & abs(cor_matrix) < 1, arr.ind = TRUE)

if (nrow(high_corr) > 0) {
  cat("There are pairs of variables with correlation coefficients greater than 0.8 or less than -0.8:\n")
  for (i in 1:nrow(high_corr)) {
    cat(rownames(cor_matrix)[high_corr[i, "row"]], "and", colnames(cor_matrix)[high_corr[i, "col"]],
        "The correlation coefficient is:", cor_matrix[high_corr[i, "row"], high_corr[i, "col"]], "\n")
  }
} else {
  cat("There are no variable pairs with correlation coefficients greater than 0.8 or less than -0.8。\n")
}
```

From the result ,we decide to remove : floor_area_sqm and lease_commence_date

```{r}
resale_tidy5_nogeo <- resale_tidy5_nogeo %>% select(-floor_area_sqm,-lease_commence_date)
```

**Check whether we have removed the high relevant variables.**

```{r}
cor_matrix <- cor(resale_tidy5_nogeo)

# Find correlation coefficients greater than 0.8 or less than -0.8 (excluding the 1s on the diagonal)
high_corr <- which(abs(cor_matrix) > 0.8 & abs(cor_matrix) < 1, arr.ind = TRUE)

if (nrow(high_corr) > 0) {
  cat("There are pairs of variables with correlation coefficients greater than 0.8 or less than -0.8:\n")
  for (i in 1:nrow(high_corr)) {
    cat(rownames(cor_matrix)[high_corr[i, "row"]], "and", colnames(cor_matrix)[high_corr[i, "col"]],
        "The correlation coefficient is:", cor_matrix[high_corr[i, "row"], high_corr[i, "col"]], "\n")
  }
} else {
  cat("There are no variable pairs with correlation coefficients greater than 0.8 or less than -0.8。\n")
}
```

**Removing the highest correlation coefficients variables from the dataset.**

```{r}
resale_tidy6 <- resale_tidy5 %>% select(-floor_area_sqm,-lease_commence_date)
```

```{r}
summary(resale_tidy6)
```

::: callout-note
The print report above reveals that variables BusStop_within_350m,Kindergartens_within_1000m are consist of 0 values which is reasonable in this case ,because some HDB may not have BusStop and Kindergartens.

However ,will notice there are some missing value in total_remaining_months and we use mean to replace them.
:::

```{r}
resale_tidy6$total_remaining_months[is.na(resale_tidy6$total_remaining_months)] <- mean(resale_tidy6$total_remaining_months, na.rm = TRUE)
```

```{r}
resale_tidy6_future <- resale_tidy5_future %>% select(-floor_area_sqm,-lease_commence_date)

resale_tidy6_future$total_remaining_months[is.na(resale_tidy6_future$total_remaining_months)] <- mean(resale_tidy6_future$total_remaining_months, na.rm = TRUE)
```

## 4.Data Sampling

```{r}
set.seed(1234) 
resale_split <- initial_split(resale_tidy6, prop = 6.5/10,) 
train_data <- training(resale_split) 
test_data <- testing(resale_split)


saveRDS(train_data, "data/rds/train_data.rds")
saveRDS(test_data, "data/rds/test_data.rds")

train_data <- readRDS("data/rds/train_data.rds")
test_data <- readRDS("data/rds/test_data.rds")

```

## 5.Building a non-spatial multiple linear regression

```{r}
price_mlr <- lm(resale_price ~ total_remaining_months +
                  storey_mid + flat_model_numeric +
                  flat_type_numeric + dist_to_elders + dist_to_hawkers +
                  dist_to_parks + dist_to_supermarkets + dist_to_chas_clinics + 
                  dist_to_cbd + dist_to_Shoppingmalls +
                  dist_to_MRT + dist_to_Goodprimaryschool +BusStop_within_350m+
                  Kindergartens_within_1000m,
                data=train_data)
summary(price_mlr)
```

From the result,except flat_model,dist_to_hawkers,dist_to_chas_clinics,dist_to_Shoppingmalls,other variables are all significant .

With dist_to_elders,dist_to_parks,dist_to_supermarkets,dist_to_cbd,dist_to_MRT,BusStop_within_350m values get higher,the dependent variable will get smaller.For example the higher distance between CBD and the HDB ,the lower house price will be.

The R-squared and adjusted R-squared of the model are high, indicating that the model has strong explanatory power for the dependent variable.

### Multicollinearity check with VIF

```{r}
vif <- performance::check_collinearity(price_mlr)
kable(vif, 
      caption = "Variance Inflation Factor (VIF) Results") %>%
  kable_styling(font_size = 18) 
```

```{r}
plot(vif) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

All VIF values ​​are less than 5, and no further multicollinearity processing is required. Overall, this is a relatively healthy model and meets expectations in terms of collinearity detection.

## 6.Preparing coordinates data

The code chunk below extract the x,y coordinates of the full, training and test data sets.

```{r}
coords <- st_coordinates(resale_tidy6)
coords_train <- st_coordinates(train_data)
coords_test <- st_coordinates(test_data)

```

```{r}
test_data_no <- test_data %>% st_drop_geometry()

resale_tidy6_future_test_data <- resale_tidy6_future %>%
  st_drop_geometry()
```

### Droping geometry field

First, we will drop geometry column of the sf data.frame by using `st_drop_geometry()` of sf package.

```{r}
train_data <- train_data %>%
  st_drop_geometry()
```

## 7.Calibrating Random Forest Model

```{r}
set.seed(1234)
rf <- ranger(resale_price ~ total_remaining_months +
                  storey_mid + flat_model_numeric +
                  flat_type_numeric + dist_to_elders + dist_to_hawkers +
                  dist_to_parks + dist_to_supermarkets + dist_to_chas_clinics + 
                  dist_to_cbd + dist_to_Shoppingmalls +
                  dist_to_MRT + dist_to_Goodprimaryschool +BusStop_within_350m+
                  Kindergartens_within_1000m,
                  data=train_data,
             importance = 'impurity')
rf
```

OOB prediction error (MSE): 1546803122. The mean square error (MSE) of the out-of-bag data (OOB) is 1546803122. MSE is used to measure the prediction error of the model. The smaller the value, the more accurate the model's prediction. OOB MSE provides an unbiased estimate of the generalization performance of the model.

R squared (OOB): 0.93448 .The R squared value of the out-of-bag data is 0.93448. R squared values ​​close to 1 indicate that the model can explain the variance of the data well. In this case, 93.448% of the variance can be explained by the model, indicating that the model performs well on the out-of-bag data.

### Extracting feature importance

```{r}
importance_rf <- rf$variable.importance

importance_df <- data.frame(Feature = names(importance_rf), 
                            Importance = importance_rf)

library(dplyr)
importance_df <- importance_df %>%
  arrange(desc(Importance))

print(importance_df)
```

```{r}
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Feature Importance for Random Forest Model (Ranger)",
       x = "Features",
       y = "Importance")
```

Feature importance analysis shows that the type of house, remaining lease time, distance to the CBD, floor and proximity to transportation hubs (such as MRT stations) have the greatest impact on HDB resale prices. Other living amenities have some impact on prices, but the effect is smaller.

## **8.Calibrating Geographical Random Forest Model**

### **Calibrating using training data**

Geographically Weighted Random Forest optimal bandwidth selection: **grf.bw**-This function finds the optimal bandwidth for the Geographically Weighted Random Forest algo-rithm using an exhaustive approach.

```{r, cache=TRUE}
#| eval: false
optimal_bw <- grf.bw(
  formula = resale_price ~ total_remaining_months +
                  storey_mid + flat_model_numeric +
                  flat_type_numeric + dist_to_elders + dist_to_hawkers +
                  dist_to_parks + dist_to_supermarkets + dist_to_chas_clinics + 
                  dist_to_cbd + dist_to_Shoppingmalls +
                  dist_to_MRT + dist_to_Goodprimaryschool +BusStop_within_350m+
                  Kindergartens_within_1000m,
  dataset = train_data,
  kernel = "adaptive",
  coords = coords_train,
  bw.min = 30,      
  bw.max = 100,     
  step = 10,          
  trees = 50,
  nthreads = 8
)

```

![](bestbandwith.png){width="312"}

From the result,we can get the best bandwidth 100 and we use this value for Geographical Random Forest Model analysis.

```{r}
#| eval: false
saveRDS(optimal_bw, "data/rds/optimal_bw.rds")
```

```{r}
optimal_bw <- readRDS("data/rds/optimal_bw.rds")
```

The code chunk below calibrate a geographic ranform forest model by using `grf()` of **SpatialML** package.

```{r}
#| eval: false
set.seed(1234)
gwRF_adaptive <- grf(formula = resale_price ~ total_remaining_months +
                  storey_mid + flat_model_numeric +
                  flat_type_numeric + dist_to_elders + dist_to_hawkers +
                  dist_to_parks + dist_to_supermarkets + dist_to_chas_clinics + 
                  dist_to_cbd + dist_to_Shoppingmalls +
                  dist_to_MRT + dist_to_Goodprimaryschool +BusStop_within_350m+
                  Kindergartens_within_1000m,
                     dframe=train_data, 
                     bw=optimal_bw, 
                     kernel="adaptive",
                     coords=coords_train)
```

::: panel-tabset
### Global Result

![](images/clipboard-2079119641.png)

![](images/clipboard-4238182932.png){width="565"}

OOB prediction error (MSE): 1350504111, this is the out-of-bag mean square error (MSE) of the model, which measures the prediction error of the model.

R squared (OOB): 0.9427949, the value of out-of-bag data, close to 1, indicating that the model has a high explanatory power on out-of-bag data.

**Flat_type_numeric** and **total_remaining_months** have the highest importance scores, indicating that these variables have a greater impact on predicting the price of second-hand houses.

**Storey_mid** and **dist_to_cbd** also have high importance, which may indicate that the floor of the house and the distance from the city center are also factors that determine the price.

Other variables (such as **dist_to_elders, dist_to_hawkers, dist_to_parks**, etc.) also contribute to the prediction, but their importance is lower.

### Local Result

![](images/clipboard-4263338467.png){width="535"}

![](截屏2024-11-07%2010.15.13.png){width="387"}

The model shows high explanatory power on the out-of-bag data (value of R squared 90.414%), but the out-of-bag mean squared error is relatively high, indicating that there may be some generalization error.

The model performs very well on the training data, with an almost perfect fit (value R squared close to 100%), but this may also indicate that the model is at risk of overfitting the training data.

### Data.frame

![](images/gwrf_d1.png)

![](images/gwrf_d2.png)

Descriptive statistics of the 15 variables in the dataframe, including minimum value (Min), maximum value (Max), mean (Mean), and standard deviation (Std)
:::

```{r}
#| eval: false
saveRDS(gwRF_adaptive, "data/rds/gwRF_adaptive.rds")
```

```{r}
gwRF_adaptive <- readRDS("data/rds/gwRF_adaptive.rds")
```

## 9.Predicting by using test data

**Multiple linear regression model**

```{r}
mlr_pred <- predict(price_mlr, test_data_no)
```

**Random Forest Model**

```{r}
rf_pred <- predict(rf,test_data_no)
```

**Geographical Random Forest Model**

Preparing the test data

```{r}
test_data_n <- cbind(test_data, coords_test) %>%
  st_drop_geometry()
```

Next, `predict.grf()` of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.

```{r}
#| eval: false
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data_n, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

```{r}
#| eval: false
saveRDS(gwRF_pred, "data/rds/gwRF_pred.rds")
```

```{r}
gwRF_pred <- readRDS("data/rds/gwRF_pred.rds")
```

**Converting the predicting output into a data frame**

The output of the `predict.grf()` is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.

```{r}
GwRF_pred_df <- as.data.frame(gwRF_pred)
```

```{r}
mlr_pred_df <- as.data.frame(mlr_pred)
```

```{r}
rf_pred_df <- as.data.frame(rf_pred)
```

## 10.Visualising the predicted values

Rename columns

```{r}
colnames(GwRF_pred_df) <- "gwRF_pred"
colnames(mlr_pred_df) <- "mlr_pred"
colnames(rf_pred_df) <- "rf_pred"
```

Bind the prediction result column to the test data

```{r}
test_data_pred <- cbind(test_data["resale_price"], mlr_pred_df, rf_pred_df, GwRF_pred_df)
```

This allows us to visually see the actual value of each data point and the predicted values ​​of different models arranged together in the same table

```{r}
print(test_data_pred)
```

## 11.Calculating Root Mean Square Error

RMSE represents the average deviation between the predicted value and the actual value. The unit is the same as the original data, so it can directly reflect the magnitude of the prediction error. The smaller the error, the lower the RMSE value, indicating that the model prediction is more accurate.

```{r}
rmse_mlr <- rmse(test_data_pred$resale_price, test_data_pred$mlr_pred)

rmse_rf <- rmse(test_data_pred$resale_price, test_data_pred$rf_pred)

rmse_gwRF <- rmse(test_data_pred$resale_price, test_data_pred$gwRF_pred)
```

## 12.Model comparison

```{r}
print(data.frame(
  Model = c("Multiple Linear Regression","Random Forest","Geographically Weighted Random Forest"),
  RMSE = c(rmse_mlr, rmse_rf,rmse_gwRF)
))
```

**Random Forest**:

RMSE = 41875.56 .The Random Forest model performs best among the three models, with the smallest RMSE value, indicating that its prediction results are closest to the actual values.

**Multiple Linear Regression**:

RMSE = 74505.25. The Multiple Linear Regression model performs second, with a higher RMSE value than the Random Forest, indicating that its prediction accuracy is lower than that of the Random Forest.

**Geographically Weighted Random Forest (GWRF)**:

RMSE = 135184.74 .The Geographically Weighted Random Forest model has the largest RMSE value, which is much higher than the other two models, indicating that its prediction effect is the worst on this dataset.

```{r}
#After confirming the test_data_pred data frame structure, run pivot_longer()
test_data_long <- test_data_pred %>%
  pivot_longer(cols = c("mlr_pred", "rf_pred","gwRF_pred"), 
               names_to = "Model", 
               values_to = "Predicted")
```

```{r}
#Using ggplot2 to draw faceted scatter plots
ggplot(data = test_data_long, aes(x = Predicted, y = resale_price)) +
  geom_point(alpha = 0.6) +
  facet_wrap(~ Model, scales = "free") +
  theme_minimal() +
  labs(title = "Model Predictions vs Actual Resale Prices",
       x = "Predicted Resale Price",
       y = "Actual Resale Price")

```

::: callout-note
A better predictive model should have the scatter point close to the diagonal line. The scatter plot can be also used to detect if any outliers in the model.
:::

**Geographically Weighted Random Forest (gwRF_pred)**: Most of the data points are concentrated near the diagonal, but there are some points that deviate from the diagonal, especially in the area of ​​high predicted values. Overall, the model is able to capture the trend of price changes, but the accuracy may be lacking in some intervals.

**Multiple Linear Regression (mlr_pred):** The distribution of predicted values ​​and actual values ​​is relatively even, but there are some points that deviate from the diagonal, especially in the high price range. This model shows a linear trend, but seems to be biased towards high prices.

**Random Forest (rf_pred):** The relationship between the predicted values ​​and actual values ​​of this model is close to the diagonal, and most of the points are distributed near the diagonal, indicating that the model is relatively accurate in predicting high and low price data. The random forest model seems to capture more price details and performs best.

So,we finally choose **Random Forest Model** to predict our future price.

## 13.Predict HDB resale prices between July-September 2024

We apply the same method to predict the price from July to September in 2024.

```{r}
rf_pred_futre <- predict(rf,resale_tidy6_future_test_data)
```

```{r}
rf_pred_futre_df <- as.data.frame(rf_pred_futre)
```

```{r}
pred_futre <- cbind(resale_tidy6_future["resale_price"], rf_pred_futre_df)
```

```{r}
print(pred_futre)
```

Now we can clearly see the result and the comparison with the actual value.

## 14.Conclusion

**Model performance**

Model performance Random Forest: The random forest model performed best among all models with the smallest RMSE value. This shows that the prediction results of the random forest model are closest to the actual values ​​and have high prediction accuracy.

Multiple Linear Regression: The RMSE value of the multiple linear regression model is higher than that of the random forest model, indicating that its prediction effect is not as good as the random forest model. Although the multiple linear regression model can better reveal the linear relationship of the data, it may be limited on complex data sets.

Geographically Weighted Random Forest (GWRF): The GWRF model has the highest RMSE value, which is much higher than the other two models, indicating that its prediction effect is the worst on this data set. The GWRF model introduced geographical weights, but failed to effectively improve the prediction effect in this data set. This may be because the role of geographical factors is not significant, and the model parameters for example the trees,the steps and the range in optimal bandwidth need further optimization.

**Feature Importance Analysis**

In terms of feature importance, variables such as **flat type** (flat_type_numeric), **remaining lease months** (total_remaining_months), and **distance to the central business district** (dist_to_cbd) have high importance in all models. This indicates that these factors are crucial in predicting HDB resale prices.

## 15.Reference

Kam, T. S. (2024). 14 Geographically Weighted Predictive Models. *R for Geospatial Data Science and Analytics.*

Kalogiou, S., & Georganos, S. (2024). *SpatialML: Spatial Machine Learning* (Version 0.1.7) \[Computer software\]. <https://stamatisgeoai.eu/>
